{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92a0d133c46f7ab2380401e3280bf661cd744731"
   },
   "source": [
    "## Triplet Model for Hampback Whole Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00f033e0e5995358433fe05bc3e064bf10e23eb0"
   },
   "source": [
    "## 5.Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import logistic\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "# from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "for i in [DeprecationWarning,FutureWarning,UserWarning]:\n",
    "    warnings.filterwarnings(\"ignore\", category = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "717414bdbd88e485356f5c411ca037969de5e59f"
   },
   "source": [
    "## 6.Define Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "embedding_dim = 50\n",
    "image_size = 224\n",
    "path_base = '../input/'\n",
    "path_train = join(path_base,'train')\n",
    "path_test = join(path_base,'test')\n",
    "path_model = join(path_base,'MyModel.hdf5')\n",
    "path_csv = '../input/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c956c38aa755e547c17569ffe9ff4fa8d0e2e035"
   },
   "source": [
    "## 7.Helping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "721a989504b5d31f9c341e1c4db86a2098c72423"
   },
   "outputs": [],
   "source": [
    "class sample_gen(object):\n",
    "    def __init__(self, file_class_mapping, other_class = \"new_whale\"):\n",
    "        self.file_class_mapping= file_class_mapping\n",
    "        self.class_to_list_files = defaultdict(list)\n",
    "        self.list_other_class = []\n",
    "        self.list_all_files = list(file_class_mapping.keys())\n",
    "        self.range_all_files = list(range(len(self.list_all_files)))\n",
    "\n",
    "        for file, class_ in file_class_mapping.items():\n",
    "            if class_ == other_class:\n",
    "                self.list_other_class.append(file)\n",
    "            else:\n",
    "                self.class_to_list_files[class_].append(file)\n",
    "\n",
    "        self.list_classes = list(set(self.file_class_mapping.values()))\n",
    "        self.range_list_classes = range(len(self.list_classes))\n",
    "        self.class_weight = np.array([len(self.class_to_list_files[class_]) for class_ in self.list_classes])\n",
    "        self.class_weight = self.class_weight/np.sum(self.class_weight)\n",
    "\n",
    "    def get_sample(self):\n",
    "        class_idx = np.random.choice(self.range_list_classes, 1, p=self.class_weight)[0]\n",
    "        examples_class_idx = np.random.choice(range(len(self.class_to_list_files[self.list_classes[class_idx]])), 2)\n",
    "        positive_example_1, positive_example_2 = \\\n",
    "            self.class_to_list_files[self.list_classes[class_idx]][examples_class_idx[0]],\\\n",
    "            self.class_to_list_files[self.list_classes[class_idx]][examples_class_idx[1]]\n",
    "\n",
    "\n",
    "        negative_example = None\n",
    "        while negative_example is None or self.file_class_mapping[negative_example] == \\\n",
    "                self.file_class_mapping[positive_example_1]:\n",
    "            negative_example_idx = np.random.choice(self.range_all_files, 1)[0]\n",
    "            negative_example = self.list_all_files[negative_example_idx]\n",
    "        return positive_example_1, negative_example, positive_example_2\n",
    "    \n",
    "    \n",
    "def read_and_resize(filepath):\n",
    "    im = Image.open((filepath)).convert('RGB')\n",
    "    im = im.resize((image_size, image_size))\n",
    "    return np.array(im, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def augment(im_array):\n",
    "    if np.random.uniform(0, 1) > 0.9:\n",
    "        im_array = np.fliplr(im_array)\n",
    "    return im_array\n",
    "\n",
    "\n",
    "def gen(triplet_gen):\n",
    "    while True:\n",
    "        list_positive_examples_1 = []\n",
    "        list_negative_examples = []\n",
    "        list_positive_examples_2 = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            positive_example_1, negative_example, positive_example_2 = triplet_gen.get_sample()\n",
    "            path_pos1 = join(path_train, positive_example_1)\n",
    "            path_neg = join(path_train, negative_example)\n",
    "            path_pos2 = join(path_train, positive_example_2)\n",
    "            \n",
    "            positive_example_1_img = read_and_resize(path_pos1)\n",
    "            negative_example_img = read_and_resize(path_neg)\n",
    "            positive_example_2_img = read_and_resize(path_pos2)\n",
    "\n",
    "            positive_example_1_img = augment(positive_example_1_img)\n",
    "            negative_example_img = augment(negative_example_img)\n",
    "            positive_example_2_img = augment(positive_example_2_img)\n",
    "            \n",
    "            list_positive_examples_1.append(positive_example_1_img)\n",
    "            list_negative_examples.append(negative_example_img)\n",
    "            list_positive_examples_2.append(positive_example_2_img)\n",
    "\n",
    "        A = preprocess_input(np.array(list_positive_examples_1))\n",
    "        B = preprocess_input(np.array(list_positive_examples_2))\n",
    "        C = preprocess_input(np.array(list_negative_examples))\n",
    "        \n",
    "        label = None\n",
    "        \n",
    "        yield ({'anchor_input': A, 'positive_input': B, 'negative_input': C}, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b219f7aa6680474d14061a20b929ea73f458c1d"
   },
   "source": [
    "## 8.Introduction to Triplet Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true,
    "_uuid": "7b35b921f62269b34f2923d65c6d65d2fb69cb6f"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('LN3RdUFPYyI', 800,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88ceb4ec20a6f8a1dae75658e16318e06e49ad97"
   },
   "source": [
    "## Concept of Triplet loss\nReferences : https://omoindrot.github.io/triplet-loss\n\nIt’s a loss function that is used when training a NN for face recognition/verification. Each training sample is actually composed of a “triplet” of images:\n* **An anchor**\n* **A positive of the same class as the anchor**\n* **A negative of a different class**\n\n![](https://omoindrot.github.io/assets/triplet_loss/triplet_loss.png)\n\nSource: [Triplet Loss and Online Triplet Mining in TensorFlow](https://omoindrot.github.io/assets/triplet_loss/triplet_loss.png)\n1. The CNN first encodes the triplets as embeddings in some vector space.\n1. You then calculate the two distances in the embedding space:\n    1. The distance between the anchor and the positive - call it d(a,p)\n    1. The distance between the anchor and the negative - call it d(a,n)\n1. You define some margin of your choice\n\nThe triplet loss is then defined as: L=max(d(a,p)−d(a,n)+margin,0)\nMinimizing it both pushes d(a,p) to 0, and d(a,n) to be bigger than d(a,p)+margin.\n\n### Triplet mining\n\nBased on the definition of the loss, there are three categories of triplets:\n\n* **easy triplets:**  triplets which have a loss of $0$, because $d(a, p) + margin < d(a,n)$\n* **hard triplets:** triplets where the negative is closer to the anchor than the positive, i.e. $d(a,n) < d(a,p)$\n* **semi-hard triplets:** triplets where the negative is not closer to the anchor than the positive, but which still have positive loss: $d(a, p) < d(a, n) < d(a, p) + margin$\n\nEach of these definitions depend on where the negative is, relatively to the anchor and positive. We can therefore extend these three categories to the negatives: hard negatives, semi-hard negatives or easy negatives.\n\nThe figure below shows the three corresponding regions of the embedding space for the negative.\n\n![](https://omoindrot.github.io/assets/triplet_loss/triplets.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "679829a47db08ed4dc97b61a5d6781f70419fa2a"
   },
   "outputs": [],
   "source": [
    "def triplet_loss(inputs, dist='sqeuclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, 1 + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(1 + K.exp(loss))\n",
    "    return K.mean(loss)\n",
    "\n",
    "def triplet_loss_np(inputs, dist='sqeuclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = np.square(anchor - positive)\n",
    "    negative_distance = np.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = np.sqrt(np.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = np.sqrt(np.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = np.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = np.sum(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = np.maximum(0.0, 1 + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = np.log(1 + np.exp(loss))\n",
    "    return np.mean(loss)\n",
    "\n",
    "def check_loss():\n",
    "    batch_size = 10\n",
    "    shape = (batch_size, 4096)\n",
    "\n",
    "    p1 = normalize(np.random.random(shape))\n",
    "    n = normalize(np.random.random(shape))\n",
    "    p2 = normalize(np.random.random(shape))\n",
    "    \n",
    "    input_tensor = [K.variable(p1), K.variable(n), K.variable(p2)]\n",
    "    out1 = K.eval(triplet_loss(input_tensor))\n",
    "    input_np = [p1, n, p2]\n",
    "    out2 = triplet_loss_np(input_np)\n",
    "\n",
    "    assert out1.shape == out2.shape\n",
    "    print(np.linalg.norm(out1))\n",
    "    print(np.linalg.norm(out2))\n",
    "    print(np.linalg.norm(out1-out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "90064c8daa36a84ca2f6b95ec540ebddb3dfdb0d"
   },
   "outputs": [],
   "source": [
    "check_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc8e689c18afdb010034fb8eda359ccba71b89f5"
   },
   "source": [
    "## 9.Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "5d2dd749451e69ac7c7cddf46e879a3715ba2e61"
   },
   "outputs": [],
   "source": [
    "def GetModel():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='max')\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = Dropout(0.6)(x)\n",
    "    x = Dense(embedding_dim)(x)\n",
    "    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n",
    "    embedding_model = Model(base_model.input, x, name=\"embedding\")\n",
    "\n",
    "    input_shape = (image_size, image_size, 3)\n",
    "    anchor_input = Input(input_shape, name='anchor_input')\n",
    "    positive_input = Input(input_shape, name='positive_input')\n",
    "    negative_input = Input(input_shape, name='negative_input')\n",
    "    anchor_embedding = embedding_model(anchor_input)\n",
    "    positive_embedding = embedding_model(positive_input)\n",
    "    negative_embedding = embedding_model(negative_input)\n",
    "\n",
    "    inputs = [anchor_input, positive_input, negative_input]\n",
    "    outputs = [anchor_embedding, positive_embedding, negative_embedding]\n",
    "       \n",
    "    triplet_model = Model(inputs, outputs)\n",
    "    triplet_model.add_loss(K.mean(triplet_loss(outputs)))\n",
    "\n",
    "    return embedding_model, triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "e467fae795f6fe546e845b1df200020301d1ed33"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_csv)\n",
    "train, test = train_test_split(data, train_size=0.7, random_state=1337)\n",
    "file_id_mapping_train = {k: v for k, v in zip(train.Image.values, train.Id.values)}\n",
    "file_id_mapping_test = {k: v for k, v in zip(test.Image.values, test.Id.values)}\n",
    "gen_tr = gen(sample_gen(file_id_mapping_train))\n",
    "gen_te = gen(sample_gen(file_id_mapping_test))\n",
    "\n",
    "checkpoint = ModelCheckpoint(path_model, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "callbacks_list = [checkpoint, early]  # early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "0cff6195879e97b150606af7721bb06c3ff40488"
   },
   "outputs": [],
   "source": [
    "def ShowImg(img):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "batch = next(gen_tr)\n",
    "\n",
    "img = batch[0]['anchor_input'][0]\n",
    "print(img.shape)\n",
    "mean = [103.939, 116.779, 123.68]\n",
    "img[..., 0] += mean[0]\n",
    "img[..., 1] += mean[1]\n",
    "img[..., 2] += mean[2]\n",
    "img = img[..., ::-1]\n",
    "ShowImg(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "368b3aae41868bf9e1b5c44cf2906be90357e248"
   },
   "source": [
    "# Installation of Resnet 50 Weight to keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "56d2f72f300261fc236286c55f7b43c6ec642fcd"
   },
   "outputs": [],
   "source": [
    "embedding_model, triplet_model = GetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "20e8f18cecce0d4ac4031b70f4f5684eec716717"
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(embedding_model.layers):\n",
    "    print(i, layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a94595ed0f78c4f252be1284b6edff4b73da2cd1"
   },
   "outputs": [],
   "source": [
    "for layer in embedding_model.layers[178:]:\n",
    "    layer.trainable = True\n",
    "for layer in embedding_model.layers[:178]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "757e049ccb357f072457432ee29a2c66b23a7a58"
   },
   "outputs": [],
   "source": [
    "triplet_model.compile(loss=None, optimizer=Adam(0.01))\n",
    "history = triplet_model.fit_generator(gen_tr, \n",
    "                              validation_data=gen_te, \n",
    "                              epochs=4, \n",
    "                              verbose=1, \n",
    "                              workers=4,\n",
    "                              steps_per_epoch=200, \n",
    "                              validation_steps=20,use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "edd76af9c13afc4672e243c569e5003ea5e7221d"
   },
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'], label='loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "def eva_plot(History, epoch):\n",
    "    plt.figure(figsize=(20,10))\n",
    "#     sns.lineplot(range(1, epoch+1), History.history['acc'], label='Train Accuracy')\n",
    "#     sns.lineplot(range(1, epoch+1), History.history['val_acc'], label='Test Accuracy')\n",
    "#     plt.legend(['train', 'validaiton'], loc='upper left')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.show()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.lineplot(range(1, epoch+1), History.history['loss'], label='Train loss')\n",
    "    sns.lineplot(range(1, epoch+1), History.history['val_loss'], label='Test loss')\n",
    "    plt.legend(['train', 'validaiton'], loc='upper left')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(\"Loss Graph\")\n",
    "    plt.show()\n",
    "    \n",
    "eva_plot(history, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "8ba76c4665ebb1d6d065c3f2902964cd0b24e73d"
   },
   "outputs": [],
   "source": [
    "for layer in embedding_model.layers[150:]:\n",
    "    layer.trainable = True\n",
    "for layer in embedding_model.layers[:150]:\n",
    "    layer.trainable = False\n",
    "triplet_model.compile(loss=None, optimizer=Adam(0.0001))\n",
    "\n",
    "history = triplet_model.fit_generator(gen_tr, \n",
    "                                    validation_data=gen_te, \n",
    "                                    epochs=3, \n",
    "                                    verbose=1, \n",
    "                                    workers=4,\n",
    "                                    steps_per_epoch=70, \n",
    "                                    validation_steps=30, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "2073383f0ac74aa66018cc3914855f46c6e73b96"
   },
   "outputs": [],
   "source": [
    "eva_plot(history, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "96a8bf7225428db200b01b5d6ead5f92b992242b"
   },
   "outputs": [],
   "source": [
    "def data_generator(fpaths, batch=16):\n",
    "    i = 0\n",
    "    imgs = []\n",
    "    fnames = []\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        img = read_and_resize(path)\n",
    "        imgs.append(img)\n",
    "        fnames.append(os.path.basename(path))\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            yield fnames, imgs\n",
    "            \n",
    "    if i != 0:\n",
    "        imgs = np.array(imgs)\n",
    "        yield fnames, imgs\n",
    "        \n",
    "    raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "93c43c75d69dd9e8f32ba4af15938d9e1d9e12da"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_csv)\n",
    "file_id_mapping = {k: v for k, v in zip(data.Image.values, data.Id.values)}\n",
    "import glob\n",
    "train_files = glob.glob(join(path_train, '*.jpg'))\n",
    "test_files = glob.glob(join(path_test, '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a5d97d6431d149d82f5166e296ab4b7285a9f48a"
   },
   "outputs": [],
   "source": [
    "train_preds  = []\n",
    "train_file_names = []\n",
    "for fnames, imgs in tqdm(data_generator(train_files, batch=32)):\n",
    "    predicts = embedding_model.predict(imgs)\n",
    "    predicts = predicts.tolist()\n",
    "    train_preds += predicts\n",
    "    train_file_names += fnames\n",
    "train_preds = np.array(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "d14eb882f2161ccd81232a23e9c3a04faaaa25c8"
   },
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_file_names = []\n",
    "for fnames, imgs in tqdm(data_generator(test_files, batch=32)) :\n",
    "    predicts = embedding_model.predict(imgs)\n",
    "    predicts = predicts.tolist()\n",
    "    test_preds += predicts\n",
    "    test_file_names += fnames\n",
    "test_preds = np.array(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "b74ad80b63d2921372ea961d08a57d21ab1f19c9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=6)\n",
    "neigh.fit(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "9b31df83a0c48c2508db42d6e2c9ed877b1dca87"
   },
   "outputs": [],
   "source": [
    "distances_test, neighbors_test = neigh.kneighbors(test_preds)\n",
    "distances_test, neighbors_test = distances_test.tolist(), neighbors_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "e72546552f519b658837252a34887cd3bfd0fd1b"
   },
   "outputs": [],
   "source": [
    "preds_str = []\n",
    "\n",
    "for filepath, distance, neighbour_ in zip(test_file_names, distances_test, neighbors_test):\n",
    "    sample_result = []\n",
    "    sample_classes = []\n",
    "    for d, n in zip(distance, neighbour_):\n",
    "        train_file = train_files[n].split(os.sep)[-1]\n",
    "        class_train = file_id_mapping[train_file]\n",
    "        sample_classes.append(class_train)\n",
    "        sample_result.append((class_train, d))\n",
    "\n",
    "    if \"new_whale\" not in sample_classes:\n",
    "        sample_result.append((\"new_whale\", 0.1))\n",
    "    sample_result.sort(key=lambda x: x[1])\n",
    "    sample_result = sample_result[:5]\n",
    "    preds_str.append(\" \".join([x[0] for x in sample_result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "15f523cd3d2d3c9d032eac1f0f563fde115ee1df"
   },
   "outputs": [],
   "source": [
    "preds_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true,
    "_uuid": "f826221d3ceb07b8991bb8cea9edcb1a3e59a434"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds_str, columns=[\"Id\"])\n",
    "df['Image'] = [x.split(os.sep)[-1] for x in test_file_names]\n",
    "df.to_csv(\"sub_humpback.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
